<!--
	Copyright 2009 The Go Authors. All rights reserved.
	Use of this source code is governed by a BSD-style
	license that can be found in the LICENSE file.
-->
<!--
	Note: Static (i.e., not template-generated) href and id
	attributes start with "pkg-" to make it impossible for
	them to conflict with generated attributes (some of which
	correspond to Go identifiers).
-->

	
		
		<div id="short-nav">
			<dl>
			<dd><code>import "."</code></dd>
			</dl>
			<dl>
			<dd><a href="#pkg-overview" class="overviewLink">Overview</a></dd>
			<dd><a href="#pkg-index" class="indexLink">Index</a></dd>
			
			
				<dd><a href="#pkg-subdirectories">Subdirectories</a></dd>
			
			</dl>
		</div>
		<!-- The package's Name is printed as title by the top-level template -->
		<div id="pkg-overview" class="toggleVisible">
			<div class="collapsed">
				<h2 class="toggleButton" title="Click to show Overview section">Overview ▹</h2>
			</div>
			<div class="expanded">
				<h2 class="toggleButton" title="Click to hide Overview section">Overview ▾</h2>
				<p>
Package ml provides some implementations of usefull machine learning
algorithms for data mining and data analysis.
</p>
<p>
The implemented algorithms are:
</p>
<pre>- Linear Regression
- Logistic Regression
- Neural Networks
</pre>
<p>
Is implemented too the fmincg function in order to calculate the optimal
theta configuration to reduce the cost value for all the implemented solutions.
</p>
<p>
Author: Alonso Vidales &lt;alonso.vidales@tras2.es&gt;
</p>
<p>
Use of this source code is governed by a BSD-style.
These programs and documents are distributed without any warranty, express or
implied. All use of these programs is entirely at the user&#39;s own risk.
</p>

			</div>
		</div>
		

		<div id="pkg-index" class="toggleVisible">
		<div class="collapsed">
			<h2 class="toggleButton" title="Click to show Index section">Index ▹</h2>
		</div>
		<div class="expanded">
			<h2 class="toggleButton" title="Click to hide Index section">Index ▾</h2>

		<!-- Table of contents for API; must be named manual-nav to turn off auto nav. -->
			<div id="manual-nav">
			<dl>
			
			
			
				
				<dd><a href="#Fmincg">func Fmincg(nn DataSet, lambda float64, length int, verbose bool) (fx []float64, i int, err error)</a></dd>
			
				
				<dd><a href="#MapFeatures">func MapFeatures(x [][]float64, degree int) (ret [][]float64)</a></dd>
			
				
				<dd><a href="#Normalize">func Normalize(values []float64) (norm []float64, valid bool)</a></dd>
			
				
				<dd><a href="#PrepareX">func PrepareX(x [][]float64, degree int) (newX [][]float64)</a></dd>
			
			
				
				<dd><a href="#DataSet">type DataSet</a></dd>
				
				
			
				
				<dd><a href="#NeuralNet">type NeuralNet</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#NewNeuralNetFromCsv">func NewNeuralNetFromCsv(xSrc string, ySrc string, thetaSrc []string) (result *NeuralNet)</a></dd>
				
				
					
					<dd>&nbsp; &nbsp; <a href="#NeuralNet.CostFunction">func (nn *NeuralNet) CostFunction(lambda float64, calcGrad bool) (j float64, grad [][][]float64, err error)</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#NeuralNet.GetPerformance">func (nn *NeuralNet) GetPerformance(verbose bool) (cost float64, performance float64)</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#NeuralNet.Hipotesis">func (nn *NeuralNet) Hipotesis(x []float64) (result []float64)</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#NeuralNet.InitializeThetas">func (nn *NeuralNet) InitializeThetas(layerSizes []int)</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#NeuralNet.MinimizeCost">func (nn *NeuralNet) MinimizeCost(maxIters int, suffleData bool, verbose bool) (finalCost float64, performance float64, trainingData *NeuralNet, testData *NeuralNet)</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#NeuralNet.SaveThetas">func (nn *NeuralNet) SaveThetas(targetDir string) (files []string)</a></dd>
				
			
				
				<dd><a href="#Regression">type Regression</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#LoadFile">func LoadFile(filePath string) (data *Regression)</a></dd>
				
				
					
					<dd>&nbsp; &nbsp; <a href="#Regression.CostFunction">func (lr *Regression) CostFunction(lambda float64, calcGrad bool) (j float64, grad [][][]float64, err error)</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#Regression.InitializeTheta">func (lr *Regression) InitializeTheta()</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#Regression.LinearHipotesis">func (data *Regression) LinearHipotesis(x []float64) (r float64)</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#Regression.LogisticHipotesis">func (data *Regression) LogisticHipotesis(x []float64) (r float64)</a></dd>
				
					
					<dd>&nbsp; &nbsp; <a href="#Regression.MinimizeCost">func (data *Regression) MinimizeCost(maxIters int, suffleData bool, verbose bool) (finalCost float64, trainingData *Regression, lambda float64, testData *Regression)</a></dd>
				
			
			
			</dl>
			</div><!-- #manual-nav -->

		

		
			<h4>Package files</h4>
			<p>
			<span style="font-size:90%">
			
				<a href="/target/fmincg.go">fmincg.go</a>
			
				<a href="/target/ml.go">ml.go</a>
			
				<a href="/target/neural_net.go">neural_net.go</a>
			
				<a href="/target/regression.go">regression.go</a>
			
			</span>
			</p>
		
		</div><!-- .expanded -->
		</div><!-- #pkg-index -->

		
		
		
			
			
			<h2 id="Fmincg">func <a href="/target/fmincg.go?s=3021:3119#L51">Fmincg</a></h2>
			<pre>func Fmincg(nn <a href="#DataSet">DataSet</a>, lambda <a href="/pkg/builtin/#float64">float64</a>, length <a href="/pkg/builtin/#int">int</a>, verbose <a href="/pkg/builtin/#bool">bool</a>) (fx []<a href="/pkg/builtin/#float64">float64</a>, i <a href="/pkg/builtin/#int">int</a>, err <a href="/pkg/builtin/#error">error</a>)</pre>
			<p>
Minimize a continuous differentialble multivariate function. Starting point
is given by the &#34;Lambda&#34; property (D by 1), and the method named &#34;CostFunction&#34;, must
return a function value and a vector of partial derivatives. The Polack-
Ribiere flavour of conjugate gradients is used to compute search directions,
and a line search using quadratic and cubic polynomial approximations and the
Wolfe-Powell stopping criteria is used together with the slope ratio method
for guessing initial step sizes. Additionally a bunch of checks are made to
make sure that exploration is taking place and that extrapolation will not
be unboundedly large. The &#34;length&#34; gives the length of the run: if it is
positive, it gives the maximum number of line searches, if negative its
absolute gives the maximum allowed number of function evaluations.
The function returns when either its length is up, or if no further
progress can be made (ie, we are at a minimum, or so close that due to
numerical problems, we cannot get any closer). If the function terminates
within a few iterations, it could be an indication that the function value
and derivatives are not consistent (ie, there may be a bug in the
implementation of your &#34;f&#34; function). The function returns &#34;fx&#34; indicating the
progress made and &#34;i&#34; the number of iterations (line searches or function evaluations,
depending on the sign of &#34;length&#34;) used.
</p>
<p>
Copyright (C) 2001 and 2002 by Carl Edward Rasmussen. Date 2002-02-13
Ported from Octave to Go by Alonso Vidales &lt;alonso.vidales@tras2.es&gt;
</p>
<p>
(C) Copyright 1999, 2000 &amp; 2001, Carl Edward Rasmussen
</p>
<p>
Permission is granted for anyone to copy, use, or modify these
programs and accompanying documents for purposes of research or
education, provided this copyright notice is retained, and note is
made of any changes that have been made.
</p>
<p>
These programs and documents are distributed without any warranty,
express or implied.  As the programs were written for research
purposes only, they have not been tested to the degree that would be
advisable in any important application.  All use of these programs is
entirely at the user&#39;s own risk.
</p>

			
		
			
			
			<h2 id="MapFeatures">func <a href="/target/ml.go?s=1827:1888#L55">MapFeatures</a></h2>
			<pre>func MapFeatures(x [][]<a href="/pkg/builtin/#float64">float64</a>, degree <a href="/pkg/builtin/#int">int</a>) (ret [][]<a href="/pkg/builtin/#float64">float64</a>)</pre>
			<p>
This method calculates all the possible combinations of the features and
returns them with the specified degree, for example, for a data.X with x1, x2
and degree 2 will convert data.X to 1, x1, x2, x1 * x2, x1 ** 2, x2 ** 2,
(x1 * x2) ** 2
Use this method with care in order to calculate the model who fits better with
the problem
</p>

			
		
			
			
			<h2 id="Normalize">func <a href="/target/ml.go?s=1036:1097#L20">Normalize</a></h2>
			<pre>func Normalize(values []<a href="/pkg/builtin/#float64">float64</a>) (norm []<a href="/pkg/builtin/#float64">float64</a>, valid <a href="/pkg/builtin/#bool">bool</a>)</pre>
			<p>
Returns all the values of the given matrix normalized, the formula applied to
all the elements is: (Xn - Avg) / (max - min) If all the elements in the
slice have the same values, or the slice is empty, the slice can&#39;t be
normalized, then returns false in the valid parameter
</p>

			
		
			
			
			<h2 id="PrepareX">func <a href="/target/ml.go?s=2506:2565#L82">PrepareX</a></h2>
			<pre>func PrepareX(x [][]<a href="/pkg/builtin/#float64">float64</a>, degree <a href="/pkg/builtin/#int">int</a>) (newX [][]<a href="/pkg/builtin/#float64">float64</a>)</pre>
			<p>
Retrns the x matrix with all the elements at the power of x, x-1, x-2, ... 1
and adds at the being of each row a 1 in order to be used as bias value
For example for a given matrix like:
</p>
<pre>3 4
5 8
</pre>
<p>
Prepared at the power of 2 (x = 2):
</p>
<pre>1 3  9 4 16
1 5 25 8 64
</pre>

			
		
		
			
			
			<h2 id="DataSet">type <a href="/target/fmincg.go?s=205:787#L1">DataSet</a></h2>
			<pre>type DataSet interface {
    <span class="comment">// Returns the cost and gradients for the current thetas configuration</span>
    CostFunction(lambda <a href="/pkg/builtin/#float64">float64</a>, calcGrad <a href="/pkg/builtin/#bool">bool</a>) (j <a href="/pkg/builtin/#float64">float64</a>, grad [][][]<a href="/pkg/builtin/#float64">float64</a>, err <a href="/pkg/builtin/#error">error</a>)
    <span class="comment">// contains filtered or unexported methods</span>
}</pre>
			<p>
Interface to be implemented by the machine learning algorithms to be used by
the Fmincg function in order to reduce the cost
</p>


			

			

			

			

			
		
			
			
			<h2 id="NeuralNet">type <a href="/target/neural_net.go?s=229:493#L6">NeuralNet</a></h2>
			<pre>type NeuralNet struct {
    <span class="comment">// Training set of values for each feature, the first dimension are the test cases</span>
    X [][]<a href="/pkg/builtin/#float64">float64</a>
    <span class="comment">// The training set with values to be predicted</span>
    Y [][]<a href="/pkg/builtin/#float64">float64</a>
    <span class="comment">// 1st dim -&gt; layer, 2nd dim -&gt; neuron, 3rd dim theta</span>
    Theta [][][]<a href="/pkg/builtin/#float64">float64</a>
}</pre>
			<p>
Neural network representation, the X and Y properties are to be used with
training proposals
</p>


			

			

			

			
				
				<h3 id="NewNeuralNetFromCsv">func <a href="/target/neural_net.go?s=8528:8617#L273">NewNeuralNetFromCsv</a></h3>
				<pre>func NewNeuralNetFromCsv(xSrc <a href="/pkg/builtin/#string">string</a>, ySrc <a href="/pkg/builtin/#string">string</a>, thetaSrc []<a href="/pkg/builtin/#string">string</a>) (result *<a href="#NeuralNet">NeuralNet</a>)</pre>
				<p>
Loads the informaton contained in the specified file paths and returns a
NeuralNet instance.
Each input file should contain a row by sample, and the values separated by a
single space.
To load the thetas specify on thetaSrc the file paths that contains each of
the layer values. The order of this paths will represent the order of the
layers.
In case of need only to load the theta paramateres, specify a empty string on
the xSrc and ySrc parameters.
</p>

				
			

			
				
				<h3 id="NeuralNet.CostFunction">func (*NeuralNet) <a href="/target/neural_net.go?s=959:1066#L22">CostFunction</a></h3>
				<pre>func (nn *<a href="#NeuralNet">NeuralNet</a>) CostFunction(lambda <a href="/pkg/builtin/#float64">float64</a>, calcGrad <a href="/pkg/builtin/#bool">bool</a>) (j <a href="/pkg/builtin/#float64">float64</a>, grad [][][]<a href="/pkg/builtin/#float64">float64</a>, err <a href="/pkg/builtin/#error">error</a>)</pre>
				<p>
Calcualtes the cost function for the training set stored in the X and Y
properties of the instance, and with the theta configuration.
The lambda parameter controls the degree of regularization (0 means
no-regularization, infinity means ignoring all input variables because all
coefficients of them will be zero)
The calcGrad param in case of true calculates the gradient in addition of the
cost, and in case of false, only calculates the cost
</p>

				
				
			
				
				<h3 id="NeuralNet.GetPerformance">func (*NeuralNet) <a href="/target/neural_net.go?s=3828:3913#L115">GetPerformance</a></h3>
				<pre>func (nn *<a href="#NeuralNet">NeuralNet</a>) GetPerformance(verbose <a href="/pkg/builtin/#bool">bool</a>) (cost <a href="/pkg/builtin/#float64">float64</a>, performance <a href="/pkg/builtin/#float64">float64</a>)</pre>
				<p>
Returns the performance of the neural network with the current set of samples.
The performance is calculated as: matches / total_samples
</p>

				
				
			
				
				<h3 id="NeuralNet.Hipotesis">func (*NeuralNet) <a href="/target/neural_net.go?s=4508:4570#L149">Hipotesis</a></h3>
				<pre>func (nn *<a href="#NeuralNet">NeuralNet</a>) Hipotesis(x []<a href="/pkg/builtin/#float64">float64</a>) (result []<a href="/pkg/builtin/#float64">float64</a>)</pre>
				<p>
Returns the hipotesis calculation for the sample &#34;x&#34; using the thetas of
nn.Theta
</p>

				
				
			
				
				<h3 id="NeuralNet.InitializeThetas">func (*NeuralNet) <a href="/target/neural_net.go?s=4968:5023#L163">InitializeThetas</a></h3>
				<pre>func (nn *<a href="#NeuralNet">NeuralNet</a>) InitializeThetas(layerSizes []<a href="/pkg/builtin/#int">int</a>)</pre>
				<p>
Random inizialization of the thetas to break the simetry.
The slice &#34;layerSizes&#34; will contain on each element, the size of the layer to
be initialized, the first layer is the input one, and last layer will
correspond to the output layer
</p>

				
				
			
				
				<h3 id="NeuralNet.MinimizeCost">func (*NeuralNet) <a href="/target/neural_net.go?s=6261:6426#L195">MinimizeCost</a></h3>
				<pre>func (nn *<a href="#NeuralNet">NeuralNet</a>) MinimizeCost(maxIters <a href="/pkg/builtin/#int">int</a>, suffleData <a href="/pkg/builtin/#bool">bool</a>, verbose <a href="/pkg/builtin/#bool">bool</a>) (finalCost <a href="/pkg/builtin/#float64">float64</a>, performance <a href="/pkg/builtin/#float64">float64</a>, trainingData *<a href="#NeuralNet">NeuralNet</a>, testData *<a href="#NeuralNet">NeuralNet</a>)</pre>
				<p>
This metod splits the samples contained in the NeuralNet instance in three
sets (60%, 20%, 20%): training, cross validation and test. In order to
calculate the optimal theta, after try with different lambda values on the
training set and compare the performance obtained with the cross validation
set to get the lambda with a better performance in the cross validation set.
After calculate the best lambda, merges the training and cross validation
sets and trains the neural network with the 80% of the samples.
The data can be shuffled in order to obtain a better distribution before
divide it in groups
</p>

				
				
			
				
				<h3 id="NeuralNet.SaveThetas">func (*NeuralNet) <a href="/target/neural_net.go?s=10474:10540#L360">SaveThetas</a></h3>
				<pre>func (nn *<a href="#NeuralNet">NeuralNet</a>) SaveThetas(targetDir <a href="/pkg/builtin/#string">string</a>) (files []<a href="/pkg/builtin/#string">string</a>)</pre>
				<p>
Store all the current theta values of the instance in the &#34;targetDir&#34; directory.
This method will create a file for each layer of theta called theta_X.txt
where X is the layer contained on the file.
</p>

				
				
			
		
			
			
			<h2 id="Regression">type <a href="/target/regression.go?s=174:539#L5">Regression</a></h2>
			<pre>type Regression struct {
    X [][]<a href="/pkg/builtin/#float64">float64</a> <span class="comment">// Training set of values for each feature, the first dimension are the test cases</span>
    Y []<a href="/pkg/builtin/#float64">float64</a>   <span class="comment">// The training set with values to be predicted</span>
    <span class="comment">// 1st dim -&gt; layer, 2nd dim -&gt; neuron, 3rd dim theta</span>
    Theta     []<a href="/pkg/builtin/#float64">float64</a>
    LinearReg <a href="/pkg/builtin/#bool">bool</a> <span class="comment">// true indicates that this is a linear regression problem, false a logistic regression one</span>
}</pre>
			<p>
Linear and logistic regression structure
</p>


			

			

			

			
				
				<h3 id="LoadFile">func <a href="/target/regression.go?s=3022:3071#L90">LoadFile</a></h3>
				<pre>func LoadFile(filePath <a href="/pkg/builtin/#string">string</a>) (data *<a href="#Regression">Regression</a>)</pre>
				<p>
Loads information from the local file located at filePath, and after parse
it, returns the Regression ready to be used with all the information loaded
The file format is:
</p>
<pre>X11 X12 ... X1N Y1
X21 X22 ... X2N Y2
... ... ... ... ..
XN1 XN2 ... XNN YN
</pre>
<p>
Note: Use a single space as separator
</p>

				
			

			
				
				<h3 id="Regression.CostFunction">func (*Regression) <a href="/target/regression.go?s=1005:1113#L20">CostFunction</a></h3>
				<pre>func (lr *<a href="#Regression">Regression</a>) CostFunction(lambda <a href="/pkg/builtin/#float64">float64</a>, calcGrad <a href="/pkg/builtin/#bool">bool</a>) (j <a href="/pkg/builtin/#float64">float64</a>, grad [][][]<a href="/pkg/builtin/#float64">float64</a>, err <a href="/pkg/builtin/#error">error</a>)</pre>
				<p>
Calcualtes the cost function for the training set stored in the X and Y
properties of the instance, and with the theta configuration.
The lambda parameter controls the degree of regularization (0 means
no-regularization, infinity means ignoring all input variables because all
coefficients of them will be zero)
The calcGrad param in case of true calculates the gradient in addition of the
cost, and in case of false, only calculates the cost
</p>

				
				
			
				
				<h3 id="Regression.InitializeTheta">func (*Regression) <a href="/target/regression.go?s=1735:1774#L48">InitializeTheta</a></h3>
				<pre>func (lr *<a href="#Regression">Regression</a>) InitializeTheta()</pre>
				<p>
Initialize the Theta property to an array of zeros with the lenght of the
number of features on the X property
</p>

				
				
			
				
				<h3 id="Regression.LinearHipotesis">func (*Regression) <a href="/target/regression.go?s=1865:1929#L53">LinearHipotesis</a></h3>
				<pre>func (data *<a href="#Regression">Regression</a>) LinearHipotesis(x []<a href="/pkg/builtin/#float64">float64</a>) (r <a href="/pkg/builtin/#float64">float64</a>)</pre>
				
				
				
			
				
				<h3 id="Regression.LogisticHipotesis">func (*Regression) <a href="/target/regression.go?s=3711:3777#L120">LogisticHipotesis</a></h3>
				<pre>func (data *<a href="#Regression">Regression</a>) LogisticHipotesis(x []<a href="/pkg/builtin/#float64">float64</a>) (r <a href="/pkg/builtin/#float64">float64</a>)</pre>
				<p>
Returns the hipotesis result for the thetas in the instance and the specified
parameters
</p>

				
				
			
				
				<h3 id="Regression.MinimizeCost">func (*Regression) <a href="/target/regression.go?s=5022:5187#L159">MinimizeCost</a></h3>
				<pre>func (data *<a href="#Regression">Regression</a>) MinimizeCost(maxIters <a href="/pkg/builtin/#int">int</a>, suffleData <a href="/pkg/builtin/#bool">bool</a>, verbose <a href="/pkg/builtin/#bool">bool</a>) (finalCost <a href="/pkg/builtin/#float64">float64</a>, trainingData *<a href="#Regression">Regression</a>, lambda <a href="/pkg/builtin/#float64">float64</a>, testData *<a href="#Regression">Regression</a>)</pre>
				<p>
This metod splits the given data in three sets: training, cross validation,
test. In order to calculate the optimal theta, tries with different
possibilities and the training data, and check the best match with the cross
validations, after obtain the best lambda, check the perfomand against the
test set of data
</p>

				
				
			
		
	

	





	
	
		<h2 id="pkg-subdirectories">Subdirectories</h2>
	
	<table class="dir">
	<tr>
	<th>Name</th>
	<th>&nbsp;&nbsp;&nbsp;&nbsp;</th>
	<th style="text-align: left; width: auto">Synopsis</th>
	</tr>
	
		<tr>
		<td><a href="..">..</a></td>
		</tr>
	
	
		
			<tr>
			<td class="name"><a href="test_data/">test_data</a></td>
			<td>&nbsp;&nbsp;&nbsp;&nbsp;</td>
			<td style="width: auto"></td>
			</tr>
		
	
	</table>
	



